---
title: "Rick e Morty"
author: "Matteo Galvan"
date: '2022-06-27'
output:
  ioslides_presentation:
    css: style.css
  slidy_presentation: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)
```

```{r, message=F, warning=F}
library(tidytext)
library(tidyverse)
library(tidygraph)
library(subtools)
library(ggplot2)
library(dplyr)
library(wordcloud)
library(igraph)
library(ggraph)
library(widyr)
library(resolution)
library(textdata)
library(reshape2)
library(wordcloud2)
library(ggpubr)
library(plotly)
library(magick)
library(topicmodels)
```

## Introduzione

Rick e Morty è una serie tv americana prodotta per Adult Swim arrivata alla quinta stagione e ancora in produzione. Il genere è il cosmic horror. 

La serie parla di Rick, nonno di Morty e padre di Beth che ritorna nella famiglia di sua figlia. Passa il suo tempo ad inventare gadget futuristici e a viaggiare in mondi e universi parlalleli prima con suo nipote Morty, e poi con sua nipote Summer.

```{r, echo=FALSE, fig.align="center"}
img = image_read("cover.jpg")
image_scale(image_scale(img,"62%"),"62%")
```

## Caratteristiche

Per effettuare questo studio sono stati recuperati i dialoghi di ogni episodio tramite i sottotitoli in inglese. 

Utilizzando un pacchetto chiamato subtools questi file sono stati convertiti in un dataset utilizzabile, formato da tutte le battute della serie. 

Il dataset è formato da 51 episodi suddivisi in 5 stagioni.

## Frequenza delle parole

Per andare a studiare la frequenza delle parole all'interno delle 5 stagioni di Rick e Morty si può utilizzare il pacchetto wordcloud2, per andare a creare una rappresentazione delle parole utilizzate maggiormente.

Le parole più usate sono rappresentate con un font del testo più grande, mentre quelle meno presenti con un font più piccolo. Inoltre se le parole sono al centro del grafico sono più importanti.

Due parole con una frequenza simile hanno uno stesso colore.


```{r, include=FALSE}
options(ggrepel.max.overlaps = Inf)
dataF <- read_subtitles_season(dir = "./srt/")
ds_noTag <- clean_tags(dataF)
ds_noCap <- clean_captions(ds_noTag)
ds_noCap <- clean_patterns(ds_noCap, "gonna")

#cerchiamo le parole
ds_singleWord <- unnest_tokens(ds_noCap, word, Text_content) %>% rename(word = "word") %>% select(Season, Episode, word) %>% anti_join(stop_words, by="word")
ds_senteces <- ds_noCap %>% rename(Sentence = "Text_content") %>% select(Season, Episode, Sentence)
```

## Frequenza delle parole
```{r , echo=FALSE, collapse=TRUE, warning=FALSE,message=FALSE}

#wordcloud_1 <- ds_singleWord %>% count(word) %>%
  #with(wordcloud(words = word, freq = n, min.freq = 1, max.words=100, random.order=FALSE, rot.per=0.35, colors=brewer.pal(8, "Dark2")))
```

```{r , echo=FALSE, collapse=TRUE, warning=FALSE,message=FALSE}
occorrences <- ds_singleWord %>% count(word) %>% arrange(-n)

wordcloud2(occorrences, size=0.9, minSize = 0.5, color='random-light', shape = 'diamond')
```

## Frequenza delle parole

```{r , echo=FALSE, collapse=TRUE, warning=FALSE,message=FALSE}
ggplot(head(occorrences, 10), aes(x = reorder(word, -n), y = n, fill=word)) + 
  geom_bar(stat = "identity", fill="#3495c9") +
  xlab("Words") +   ylab("Occurrences")
```

## Frequenza delle parole

I due termini maggiormente presenti sono il nome di Rick e di Morty che sostanzialmente sono i protagonisti di ogni episodio. 

Inoltre in molti episodi esistono più versioni di Morty e di Rick che si nominano a vicenda e questo comporta che ci sia una maggior presenza di questi termini.

Gli altri termini molto presenti sono le esclamazioni che il nonno di Morty pronuncia come un intercalare.

## Densità dei termini

Quale densità e che distribuzione di frequenza assoluta possiedono i termini che compaiono nelle 5 stagioni?

```{r , echo=FALSE, collapse=TRUE, warning=FALSE,message=FALSE}

density <- ggplot(occorrences, aes(x=n)) + geom_density(alpha=.3, color="darkblue") +
  theme(panel.border = element_rect(colour = "black", fill=NA, size=0.5))  +
  xlab("Words") + ylab("Density")

freqAss <- ggplot(occorrences, aes(x=word, y=n, label=word)) + 
  geom_point(aes(colour = cut(n, c(-Inf, 300, Inf))),
             size = 2, show.legend=F) +
  scale_color_manual(name = "n",
                     values = c("(-Inf,300]" = "black",
                                  "(300, Inf]" = "red")) +
  theme(axis.text.x=element_blank(),
        axis.ticks.x=element_blank(),
        panel.grid.major = element_blank(), 
        panel.grid.minor = element_blank(),
        panel.border = element_rect(colour = "black", fill=NA, size=0.5))+
  geom_text(aes(label=ifelse(n>300,as.character(word),'')),hjust=1,vjust=1.1)  +
  xlab("Words") +   ylab("Occurrences")

ggarrange(density, freqAss,
          ncol = 2, nrow = 1)
```

Il primo grafico rappresenta con quale densità compaiono i termini e vediamo che è un'iperbole in cui pochi termini compaiono molte volte e tanti termini compaiono poco.

Nel secondo grafico verifichiamo la stessa cosa, osservando una grande quantità di punti rappresentanti le parole che hanno una frequenza assoluta molto bassa, e pochi termini, tipo il termine "Rick" e "Morty" che compaiono molto di più.

## Rick

```{r , echo=FALSE, collapse=TRUE, warning=FALSE,message=FALSE}
freq <- ds_singleWord %>% filter(word == "rick") %>%
  count(Season, Episode)

ggplot(freq)+
  geom_bar(aes(y = n, x = Episode, fill = Episode), stat = "identity", show.legend=F)+
  ylab("Occurrences")+
  xlab("Episodi")+
  ggtitle("Occorrrenze di Rick") +
  facet_wrap(~Season, scales = 'free_x', ncol=5) +
  theme(panel.spacing = unit(0, 'lines')) +
  scale_x_continuous(breaks=seq(0,11,by=1)) +
  theme(text = element_text(size=10))
```
Possiamo notare come per esempio nell'episodio 1x10, si dica molto la parola Rick. Infatti questo episodio nominato "Rick e Summer" parla d una festa organizzata da Rick in cui invita degli amici a casa sua.

Nell'episodio 3x07 viene pronunciata molto la parola Rick. Infatti questo episodio nominato "A Ricklantide" è ambientato nella città dei Rick in cui sono presenti moltissime versioni dello stesso uomo ma di molti universi paralleli.

## Summer

```{r , echo=FALSE, collapse=TRUE, warning=FALSE,message=FALSE}
freq <- ds_singleWord %>% filter(word == "summer") %>%
  count(Season, Episode)

ggplot(freq)+
  geom_bar(aes(y = n, x = Episode, fill = Episode), stat = "identity", show.legend=F)+
  ylab("Occurrences")+
  xlab("Episodi")+
  ggtitle("Occorrrenze di Summer") +
  facet_wrap(~Season, scales = 'free_x', ncol=5) +
  theme(panel.spacing = unit(0, 'lines')) +
  scale_x_continuous(breaks=seq(0,11,by=1)) +
  theme(text = element_text(size=10))
```

Osservando le frequenze del nome Summer, nell'episodio 2x07, Summer chiede aiuto a suo nonno Rick per uccidere un vampiro. Rispetto ad altri episodi il nome di Summer compare di più perchè è ambientato nella scuola che lei frequenta.

## Bigram analysis

Un bigram è una sequenza di due elementi adiacenti in una stringa di token, che in questo caso sono parole.

I bigram sono utilizzabili per andare a studiare la correlazione tra termini e per verificare quali sono le parole che sono maggiormente collegate alle altre.

```{r , echo=FALSE, collapse=TRUE, warning=FALSE,message=FALSE}
bigrams <- unnest_tokens(ds_senteces, bigram, Sentence, token = "ngrams", n = 2)
bigrams <- bigrams %>% filter(!is.na(bigram)) %>% filter(bigram != "ah ah", bigram != "ha ha")

#bigrams %>% count(bigram, sort = TRUE)

bigrams_separated <- bigrams %>%
  separate(bigram, c("word1", "word2"), sep = " ")

bigrams_filtered <- bigrams_separated %>%
  filter(!word1 %in% stop_words$word) %>%
  filter(!word2 %in% stop_words$word)

# new bigram counts
bigram_counts <- bigrams_filtered %>% 
  count(word1, word2, sort = TRUE)

#bigram_counts

bigrams_united <- bigrams_filtered %>%
  unite(bigram, word1, word2, sep = " ")

#bigrams_united

bigram_counts2 <- bigrams_united %>% 
  count(bigram, sort = TRUE)

#bigram_counts2

```
## Bigram analysis

Quali sono i bigram maggiormente presenti?

```{r, echo=FALSE, collapse=TRUE, warning=FALSE,message=FALSE}
bigrams_for_plot = bigrams_united %>% semi_join(head(bigram_counts2, 10), by="bigram") %>%
  count(bigram, Season, sort=TRUE)

xform <- list(categoryorder = "array",
              categoryarray = unique(bigrams_for_plot$chapter))

bigrams_plot <- plot_ly(bigrams_for_plot, x = ~Season, y = ~n, type = 'bar', color = ~bigram)
bigrams_plot <- bigrams_plot %>% layout(yaxis = list(title = 'number of bigrams'), barmode = 'stack', xaxis = xform)  

#bigrams_plot
ggplot(bigrams_for_plot, aes(fill=bigram, y=n, x=Season)) + 
    geom_bar(position="stack", stat="identity", lwd=0.001, color="grey") +
    scale_fill_manual(values = c("god damn" = "#31eab8",
                               "grandpa rick" = "#40d6b5",
                               "hey hey" = "#4fc1b1",
                               "hey rick" = "#5eadae",
                               "holy shit" = "#6d98aa",
                               "jesus christ" = "#7b84a7",
                               "rick sanchez" = "#8a6fa3",
                               "wait wait" = "#995ba0",
                               "whoa whoa" = "#a8469c",
                               "yeah yeah" = "#b73299")) +
                                xlab("Season") + ylab("Occurrences")
```


## Bigram analysis

Il grafico è stato diviso per stagione e rappresenta la frequenza assoluta dei 10 bigram maggiormente presenti nei dialoghi.

Si nota che i bigram maggiormente utilizzati, oltre ai nomi quali "Rick Sanchez", "Hey Rick" e "Grandpa Rick", sono tutte esclamazioni.

Se si osserva la frequenza con cui queste esclamazioni compaiono si vede che nelle ultime stagione è stato fatto un uso maggiore di esclamazioni negative, rispetto a quelle innocue come "yeah yeah".

## Frequenza bigram in base al tf-idf

I bigram possono essere visti come singoli termini e per ogni termine si può calcolare il tf-idf come misurazione per capire quanto un termine è discriminante di un testo, in questo caso delle stagioni.

```{r , echo=FALSE, collapse=TRUE, warning=FALSE,message=FALSE}

bigram_tf_idf <- bigrams_united %>%
  count(Season, bigram) %>%
  bind_tf_idf(bigram, Season, n) %>%
  arrange(desc(tf_idf))

bigram_tf_idf %>%
  arrange(desc(tf_idf)) %>%
  group_by(Season) %>%
  top_n(5, tf_idf) %>%
  ungroup() %>%
  mutate(bigram = reorder(bigram, tf_idf)) %>%
  ggplot(aes(bigram, tf_idf, fill = Season)) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~ Season, ncol = 2, scales = "free") +
  coord_flip() +
  labs(y = "tf-idf of bigram to season",
       x = "")
```

## Frequenza bigram in base al tf-idf
Dal grafico, diviso per stagioni, si vede che i bigram maggiomente presenti sono quelli che discriminano maggiormente la stagione. 

Nella stagione 2 è presente il bigram mini Rick che compare molte volte all'interno di un singolo episodio e questo lo rende un termine molto importante.

Nelle altre stagioni non c'è una grande rilevanza di termini.


## Grafo

Rappresentazione grafica della connessione dei termini. 

```{r , echo=FALSE, collapse=TRUE, warning=FALSE,message=FALSE}
bigram_graph <- bigram_counts %>%
  #filter(n > 4) %>% 
  as_tbl_graph()

a <- grid::arrow(type = "closed", length = unit(.1, "inches"))

# plot the graph
grafico <- ggraph(bigram_graph, layout = "fr") +
  geom_edge_link(aes(edge_alpha = n), show.legend = FALSE,
                 arrow = a, end_cap = circle(.07, 'inches')) +
  geom_node_point(color = "lightblue", size = 1) +
  geom_node_text(aes(label = name), vjust = 1, hjust = 1) +
  theme_void()

g <- head(bigram_counts, 1000) %>%
  graph_from_data_frame() %>%
  ggraph(layout = "fr") +
  geom_edge_link0()+
  geom_node_point(color = "lightblue", size = 1)
g + theme_graph(background = "white")
```

Nel grafo è presente una componente gigante?
```{r , echo=FALSE, collapse=TRUE, warning=FALSE,message=FALSE}
struct <- bigram_counts %>%
  graph_from_data_frame() 
options(ggrepel.max.overlaps = Inf)

c = components(struct)
nodes = which(c$membership == which.max(c$csize))

V(struct)$color = "white"
V(struct)[nodes]$color = "black"
#plot(struct, vertex.size = 3, vertex.label=NA, edge.arrow.size=0.005, edge.arrow.width=0.0000001)
```

Il grafo mostra la componente gigante formata dai bigram collegati tra di essi.
Esiste quindi una componente gigante che prende la maggior parte di nodi. Il numero di nodi connessi è: 6619
```{r}
#vcount(struct)-c$no
```


## Misure di Centralità
```{r}
paths = distance_table(struct)$res
names(paths) = 1:length(paths)

C <- transitivity(struct, type="global")
```
Che misurazioni ha il grafo?

Numero di nodi: 7106
```{r}
#vcount(struct)
```

Distanza media tra due nodi: 5.46047
```{r}
#mean_distance(struct)
```

Diametro: 19
```{r}
#diameter(struct)
```

Centralità: 0.02401172
```{r}
#C
```
## Cammino medio tra due nodi

Quale è la distribuzione della distanza media tra due nodi qualsiasi?
```{r}
barplot(paths / sum(paths), xlab="Distance", ylab="Frequence", col=rgb(0.2,0.4,0.6,0.6))
```

Si può notare che seppur la grandezza della rete sia abbastanza elevata (7106 nodi), in realtà la distanza media tra due nodi qualsiasi è molto contenuta. 

Il diametro, cioè la geodesica più lunga è di 19 passi.
La centralità di vicinanza ha un valore molto basso, che è dovuto principalmente ai nodi che compaiono singolarmente, magari all'interno di una singola battuta effettuata da un personaggio.

## Pairwise
Oltre a vedere le coppie di termini più frequenti è possibile andare a studiare le parole che possiedono maggior correlazione, perchè compaiono nella stessa situazione ma non sono per forza adiacenti.

Il grafo rappresenta i termini come nodi e lo spessore dell'arco che li collega come relazione tra di essi. I nodi in rosso sono i termini che hanno una frequenza assoluta maggiore all'interno dei dialoghi.

## Pairwise
```{r , echo=FALSE, collapse=TRUE, warning=FALSE,message=FALSE}
word_cor <- ds_singleWord %>% filter(word != "ah", word != "ha") %>% mutate(Sigla = paste(Season, Episode)) %>%
  add_count(word, Sigla) %>%
  filter(n >= 15) %>%
  arrange(-n)

word_cor <- distinct(word_cor)
word_cor <- word_cor %>% pairwise_cor(word, Sigla, sort = TRUE)
```
```{r , echo=FALSE, collapse=TRUE, warning=FALSE,message=FALSE}
struct <- word_cor %>%
  filter(correlation > .1) %>%
  as_tbl_graph() %>%
  ggraph(layout = "star")

mcolor <- struct$data %>% mutate(mcolor = if_else(name %in% c("morty", "rick", "yeah", "hey"), 
                                     "#e47a38", "#174ECF")) %>% select(mcolor)

struct +
  geom_edge_link(aes(edge_alpha = correlation), show.legend = FALSE) +
  geom_node_point(colour=mcolor$mcolor, size = 3)+
  geom_node_text(aes(label = name), repel = TRUE, colour=mcolor$mcolor)
```

Si può vedere che i 4 termini non hanno una relazione molto elevata con gli altri presenti all'interno del testo. 


## Betweenness centrality
Quali sono le parole che hanno un valore di correlazione maggiore se comparate a tutte le altre parole?

Calcoliamo la centralità betweenness che va a controllare quanto una parola compara in mezzo ad altre. In sostanza va a verificare se il termine appare sulla maggior parte delle geodesiche presenti tra due termini.

## Betweenness centrality
```{r , echo=FALSE, collapse=TRUE, warning=FALSE,message=FALSE}
word_cor <- ds_singleWord %>% filter(word != "ah", word != "ha") %>% mutate(Sigla = paste(Season, Episode)) %>%
  add_count(word, Sigla) %>%
  filter(n >= 10) %>%
  arrange(-n)

word_cor <- distinct(word_cor)
word_cor <- word_cor %>% pairwise_cor(word, Sigla, sort = TRUE)

word_cor_g <- word_cor %>%
  rename(word1 = item1, word2 = item2, n = correlation) %>%
  filter(n > 0.18)

#Betweenness centrality
g <- word_cor_g %>%
  as_tbl_graph()

v <- as_tibble(g) %>%
  mutate(v = row_number())

b <- betweenness(g)

names(b) = 1:vcount(g)

betweenness <- data.frame(score = round(b, 2)) %>%
  mutate(v = row_number()) %>%
  full_join(v, by="v") %>%
  arrange(desc(score)) %>%
  mutate(word = name) %>%
  select(word, score) %>% 
  head(10)

betweenness %>% rmarkdown::paged_table()

#PageRank centrality
#pr <- page_rank(g)
#pagerank <- data.frame(score = pr$vector) %>%
 # arrange(desc(score)) %>%
 # head(10)

#pagerank
```

## Community detection
Il community detection permette di trovare le divisioni naturali di una rete in un gruppo di vertici connessi, chiamate comunità. Tra queste comunità ci sono pochi archi, mentre ce ne sono un numero maggiore all'interno della comunità.

Osserviamo i termini che secondo la centralità sono maggiormente importanti:

* uh
* shit
* jerry
* god

```{r , echo=FALSE, collapse=TRUE, warning=FALSE,message=FALSE}
word_cor <- ds_singleWord %>% filter(word != "ah", word != "ha") %>% mutate(Sigla = paste(Season, Episode)) %>%
  add_count(word, Sigla) %>%
  filter(n >= 13) %>%
  arrange(-n)

word_cor <- distinct(word_cor)
word_cor <- word_cor %>% pairwise_cor(word, Sigla, sort = TRUE)
word_cor_g <- word_cor %>%
  rename(word1 = item1, word2 = item2, n = correlation) %>%
  filter(n > 0.05)

g <- word_cor_g %>% 
  filter(word1 == "uh" | word2 == "uh" | word1 == "shit" | word2 == "shit" | word1 == "jerry" | word2 == "jerry"| word1 == "god" | word2 == "god")

G <- graph_from_data_frame(g)
community <- cluster_resolution(G, t = 1.5) # The number of communities typically decreases as the resolution parameter (t) grows.
```
## Community detection

```{css, echo=FALSE}
.centered {
  position: absolute;
  top: 0px;
}
```

<div class="centered">
```{r, echo=FALSE, warning=FALSE, fig.width=8, fig.height=8}
plot(G, vertex.color = membership(community), vertex.size = 10, edge.arrow.size = 0.05, edge.arrow.width=0.00001, layout=layout.lgl(G), vertex.label.dist=0, vertex.label.cex=0.7)

#legend("right",inset = c(0, 0),legend = c("yeah","uh", "god", "hey"), lty = 1, col = c("#FFFF00", "#008800", "#FFBB00", "#8888ff"),lwd =2,xpd = TRUE)
```
</div>

Possiamo notare come questi 4 termini siano legati tra di loro, osservando in maniera più dettagliata che sono legate a parole negative.

## Sentiment Analisy
Cerchiamo di effettuare un'analisi sui sentimenti e le emozioni che questa serie tv possono trasmettere a coloro che la guardano.

Per l'analisi utilizzero 3 diverse scale di misura:

* bing: ha un controllo sui termini booleano che verifica se sono positivi o negativi

* nrc: permette di riconoscere le emozioni in maniera più dettagliata 

* afinn: permette di dare una valutazione ad ogni termine che va da -5 a 5.

## Wordcloud
Quali sono le parole che influenzano in maniera maggiore o minore i sentimenti in questa analisi?
```{r , echo=FALSE, collapse=TRUE, warning=FALSE,message=FALSE}
bing <- get_sentiments("bing")

ds_singleWord %>% 
  inner_join(bing, "word") %>%
  count(word, sentiment, sort=T) %>% 
  acast(word ~ sentiment, value.var = "n", fill=0) %>% 
  comparison.cloud(colors=c("#991D1D", "#327CDE"), max.words = 100)
```
Notiamo che per quanto riguarda le parole negative abbiamo una prevalenza di escalmazioni o di aggettivi positivi come love, whoa e cool.

Per quanto riguarda le parole negativi abbiamo delle parolacce come fucking, shit oppure fuck, che vengono pronunciate per la maggior parte da Rick ma in seguito molto anche da Morty.

## Divisione dei sentimenti
Quali sono le emozioni più presenti, e quale è il numero di parole che caratterizzano queste emozioni?

```{r , echo=FALSE, collapse=TRUE, warning=FALSE,message=FALSE}
nrc <- get_sentiments("nrc")
sentiments <- ds_singleWord %>% 
  filter(word != "god") %>%
  inner_join(nrc, "word") %>%
  count(sentiment, sort=T)

sentiments %>% 
  ggplot(aes(x=reorder(sentiment, n), y=n)) +
  geom_bar(stat="identity", aes(fill=sentiment), show.legend=F) +
  geom_label(label=sentiments$n) +
  labs(x="Sentiment", y="Frequency", title="How is the overall mood in Rick&Morty?") +
  coord_flip()
```
Possiamo notare come in un classificatore binario le parole negative e positive siano più o meno lo stesso numero. Si vede subito che le emozioni più presenti all'interno della serie tv sono la fiduzia, la paura e altre emozioni più negative. 

## Divisione dei sentimenti
Quali sono i termini che caratterizzano le emozioni?

```{r , echo=FALSE, collapse=TRUE, warning=FALSE,message=FALSE}
ds_singleWord %>% 
  filter(word != "god") %>%
  inner_join(nrc, "word") %>% 
  count(sentiment, word, sort=T) %>% 
  group_by(sentiment) %>% 
  arrange(desc(n)) %>% 
  slice(1:7) %>% 
  ggplot(aes(x=reorder(word, n), y=n)) +
  geom_col(aes(fill=sentiment), show.legend = F) +
  facet_wrap(~sentiment, scales = "free_y") +
  theme(axis.text.x = element_text(angle=45, hjust=1)) +
  coord_flip() +
  theme_bw() +
  labs(x="Word", y="Frequency")
```

## Divisione dei sentimenti
Notiamo come le parolacce e le parole negative tipo "pistola", "morte", "uccisione" siano molto presenti. 

La serie tv è infatti abbastanza violeta e le scene di sparatorie sono molto comuni, come le scene in cui vengono usate delle pistole.

## Afinn
Vediamo ora le parole che globalmente danno il più grande contributo sul lato positivo e negativo.

```{r , echo=FALSE, collapse=TRUE, warning=FALSE,message=FALSE}
afinn <- get_sentiments("afinn")

ds_singleWord %>% 
  filter(word != "god") %>%
  inner_join(afinn, "word") %>% 
  count(word, value, sort=T) %>% 
  mutate(contribution = n * value,
         sentiment = ifelse(contribution<=0, "Negative", "Positive")) %>% 
  arrange(desc(abs(contribution))) %>% 
  head(20) %>% 
  
  ggplot(aes(x=reorder(word, contribution), y=contribution, fill=sentiment)) +
  geom_col(aes(fill=sentiment), show.legend = F) +
  labs(x="Word", y="Contribution") +
  coord_flip() +
  scale_fill_manual(values=c("#FA8072", "#08439A")) + 
  theme_bw()
```

Questo grafico conferma ulteriormente che le parolacce rivestono un ruolo molto importante perchè parole come shit, fuck o hell pesano in modo molto alto quando andiamo a fare un resoconto.

## Sentiment Analisy - Season
Osservando i singoli episodi delle 5 stagioni, come è l'andamento dei sentimenti?

```{r , echo=FALSE, collapse=TRUE, warning=FALSE,message=FALSE}
rick_morty_sentiment2 <- ds_singleWord %>% filter(word != "god") %>%
  inner_join(get_sentiments("bing"), by="word") %>%
  count(Season, index=Episode, sentiment) %>%
  spread(sentiment, n, fill = 0) %>%
  mutate(sentiment = positive - negative)

ggplot(rick_morty_sentiment2, aes(index, sentiment, fill = Season)) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~Season, ncol = 2, scales = "free_x")  +
  ylab("Frequence") + xlab("Season/Episode")  +
  scale_x_continuous(breaks=seq(0,11,by=1)) +
  theme(text = element_text(size=10))
```
Osserviamo che l'andamento di tutti gli episodi è negativo. Le parolacce nei dialoghi rendono la serie tv molto negativa. 

L'episodio 2x05 è l'unico sommariamente positivo e parla di una gara di canto interplanetaria in cui è presente anche il presidente degli Stati Uniti. La parola "president" oltre alle parole delle canzoni, rendono questo episodio migliore di altri. Seppur nell'episodio certi pianeti vengano distrutti.

L'episodio 3x06 è l'episodio più negativo: parla di un viaggio rilassante alle terme in cui però la parte cattiva di Rick e poi quella di Morty scappano e quella di Morty vuole intossicare il mondo. Rappresenta proprio la negatività.

## Topic modeling
Possiamo vedere i dialoghi degli episodi e delle stagioni come un insieme di argomenti che sono formati da parole.

Quali sono gli argomenti principali che caratterizzano la serie tv?

Utilizziamo il pacchetto topicmodels che permette di calcolare la probabilità che un termine cada in uno specifico topic.
Estraiamo 3 argomennti dagli episodi della prima stagione e recuperiamo le 10 parole con la probabilità maggiore.

```{r, echo=FALSE, collapse=TRUE, warning=FALSE}
contatore = ds_singleWord %>%
  count(Episode, word, sort = TRUE)

# cast into DTM
seasEpi = contatore %>%
  cast_dtm(Episode, word, n)

# create 3 topics with LDA
SeasEpi_lda = LDA(seasEpi, k = 3, control = list(seed = 1234))
```

## Topic modeling

```{r echo=FALSE, collapse=TRUE, warning=FALSE}
top <- tidy(SeasEpi_lda, matrix = "beta") %>%
  group_by(topic) %>%
  top_n(10, beta) %>%
  ungroup() %>%
  arrange(topic, -beta)

top %>%
  mutate(termini = reorder_within(term, beta, topic)) %>%
  ggplot(aes(termini, beta, fill = factor(topic))) +
  geom_col(show.legend = FALSE) +
  coord_flip() +
  scale_x_reordered() +
  scale_fill_manual(values=c("#fe34a3", "#44c1bb", "#c5bffb"))+
  facet_wrap(~ topic, scales = "free")  +
  xlab("Words") +   ylab("Beta")
```


Come si pò notare dal grafico non c'è una particolare suddivisione tra i topic. Infatti le parole che compaiono sono sempre le stesse e questo non permette di trovare degli argomenti interessanti per suddividere le stagioni o gli episodi. 

## Conclusione

Inizialmente è stata effettuata una analisi sulle caratteristiche del dataset, in seguito sulla correlazione tra i termini, osservando quelli con maggiore importanza tramite dei grafici.

In seguito è stato analizzato il grafo risultante dalle misurazioni della rete.

Uno studio più articolato ha permesso di apprendere in maniera dettagliata i sentimenti che questa serie tv trasmette e che la caratterizzano pienamente.

Infine uno studio tramite LDA ci ha permesso di capire se effettivamente è possibile cogliere il topic degli episodi.
